<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Short bio on Luis Damiano</title>
    <link>/</link>
    <description>Recent content in Short bio on Luis Damiano</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep GPs: summary of Deep Gaussian Processes by Damiano &amp; Lawerence (2013)</title>
      <link>/blog/deep-gps-summary-of-deep-gaussian-processes-by-damiano-lawerence-2013/</link>
      <pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/deep-gps-summary-of-deep-gaussian-processes-by-damiano-lawerence-2013/</guid>
      <description>This is a quick, math-free summary of Deep Gaussian Processes by Damiano &amp;amp; Lawrence (2013). Many details are purposedly left out to lighten the read, see the full paper.
Disclaimer: As this is a first draft that has not been fully edited yet, watch out for errors, inaccuracies, and unfortunate choices of technical vocabulary. Let me know if you find any of these and I&amp;rsquo;ll get them fixed.
About the article  Main idea: introduce GP Gaussian Processes for learning tasks.</description>
    </item>
    
    <item>
      <title>Feature engineering for machine learning</title>
      <link>/blog/feature-engineering-for-machine-learning/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/feature-engineering-for-machine-learning/</guid>
      <description>Introduction They say [citation needed] that feature engineering is key for prediction and one should rather have a simple enough model fed with solid inputs. For instance, well-crafted nonlinear features could take the burden off nonlinear models. This is a rough and high-level list of the feature engineering strategies I&amp;rsquo;ve found. Mainly, I&amp;rsquo;m looking for when, pros, cons, recommendations, and warnings. Here definitions and explanations are intentionally omitted as they can be readily looked up on demand.</description>
    </item>
    
    <item>
      <title>Academic work</title>
      <link>/research/work/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/research/work/</guid>
      <description>Academic Work This short list excludes in printing and in progress work. A detailed Curriculum Vitae is available here.
BayesHMM: Full Bayesian Inference for Hidden Markov Models April 2019 - A poster presented at the 44th Spring Lecture Series, University of Arkansas.
 Poster: view online.  bayesdfa: Bayesian Dynamic Factor Analysis (DFA) with &amp;lsquo;Stan&amp;rsquo; September 2018 - An R Package available on CRAN.
 CRAN: view online.  BayesHMM: An R Package for Full Bayesian Inference for Hidden Markov Models August 2018 - Google Summer of Code 2018 project.</description>
    </item>
    
  </channel>
</rss>