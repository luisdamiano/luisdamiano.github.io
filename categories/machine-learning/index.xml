<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Luis Damiano</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Luis Damiano</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Feature engineering for machine learning</title>
      <link>/blog/feature-engineering-for-machine-learning/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/feature-engineering-for-machine-learning/</guid>
      <description>Introduction They say [citation needed] that feature engineering is key for prediction and one should rather have a simple enough model fed with solid inputs. For instance, well-crafted nonlinear features could take the burden off nonlinear models. This is a rough and high-level list of the feature engineering strategies I&amp;rsquo;ve found. Mainly, I&amp;rsquo;m looking for when, pros, cons, recommendations, and warnings. Here definitions and explanations are intentionally omitted as they can be readily looked up on demand.</description>
    </item>
    
  </channel>
</rss>